{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5afdfac3-5e76-4f3f-8a82-0e4f95df7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert isinstance(power, (int, float))\n",
    "        out = Value(self.data ** power, (self,), f'**{power}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += power * (self.data ** (power - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        t = math.tanh(self.data)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86c1e71c-9c9e-44aa-8295-bc8ae4dc2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh(x) = 0.9640275800758169\n",
      "dy/dx = 0.07065082485316443\n"
     ]
    }
   ],
   "source": [
    "x = Value(2.0)\n",
    "y = x.tanh()\n",
    "y.backward()\n",
    "\n",
    "print(\"tanh(x) =\", y.data)\n",
    "print(\"dy/dx =\", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcdf06b3-34af-4c54-aca1-2ba0dd00b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9bf6449-0881-4c90-89f3-2d67bb4dc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.tanh() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'tanh' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5f03f75-2181-4581-af1d-deafc164168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph(v, indent=0, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if v in visited:\n",
    "        return\n",
    "    visited.add(v)\n",
    "    print(\"  \" * indent + f\"{v._op} | data={v.data:.4f} | grad={v.grad:.4f}\")                         #Graphviz wasnt working \n",
    "    for child in v._prev:\n",
    "        print_graph(child, indent + 1, visited)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8b8bbcc-e163-4376-942d-29ef042bd9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* | data=0.0058 | grad=1.0000\n",
      "  + | data=0.0230 | grad=0.5000\n",
      "    + | data=0.0230 | grad=0.7500\n",
      "      **2 | data=0.0009 | grad=1.0000\n",
      "        + | data=0.0305 | grad=0.0764\n",
      "          * | data=-1.0000 | grad=0.0916\n",
      "             | data=1.0000 | grad=0.6774\n",
      "             | data=-1.0000 | grad=0.1069\n",
      "          + | data=1.0305 | grad=0.0916\n",
      "            + | data=0.7646 | grad=0.1069\n",
      "              + | data=0.7696 | grad=0.1222\n",
      "                * | data=0.8059 | grad=0.1375\n",
      "                  tanh | data=-0.9695 | grad=-0.1275\n",
      "                    + | data=-2.0832 | grad=-0.0084\n",
      "                      * | data=-1.7554 | grad=-0.0092\n",
      "                         | data=2.0000 | grad=-0.0031\n",
      "                         | data=-0.8788 | grad=0.3131\n",
      "                      + | data=-0.3278 | grad=-0.0092\n",
      "                        * | data=-0.3445 | grad=-0.0100\n",
      "                           | data=0.3448 | grad=-0.1049\n",
      "                           | data=-1.0000 | grad=0.1527\n",
      "                         | data=0.0173 | grad=-0.1616\n",
      "                   | data=-0.8351 | grad=0.9132\n",
      "                + | data=-0.0363 | grad=0.1375\n",
      "                  * | data=0.0060 | grad=0.1527\n",
      "                     | data=-0.1573 | grad=0.3329\n",
      "                    tanh | data=-0.0385 | grad=-0.0264\n",
      "                      + | data=-0.0385 | grad=-0.0287\n",
      "                        * | data=-0.2717 | grad=-0.0311\n",
      "                           | data=-0.1366 | grad=0.2440\n",
      "                        + | data=0.2332 | grad=-0.0311\n",
      "                           | data=0.0057 | grad=-0.1132\n",
      "                          * | data=0.2280 | grad=-0.0335\n",
      "                             | data=-0.2280 | grad=0.0104\n",
      "                   | data=-0.0438 | grad=0.2611\n",
      "              * | data=-0.0049 | grad=0.1222\n",
      "                 | data=0.2196 | grad=-0.2533\n",
      "                tanh | data=-0.0226 | grad=0.0302\n",
      "                  + | data=-0.0226 | grad=0.0335\n",
      "                    * | data=0.8271 | grad=0.0368\n",
      "                       | data=0.4138 | grad=-0.0667\n",
      "                    + | data=-0.8497 | grad=0.0368\n",
      "                       | data=-0.0088 | grad=0.1808\n",
      "                      * | data=-0.8416 | grad=0.0402\n",
      "                         | data=0.8413 | grad=0.1063\n",
      "            * | data=0.2659 | grad=0.1069\n",
      "               | data=-0.3169 | grad=0.0581\n",
      "              tanh | data=-0.8403 | grad=-0.0387\n",
      "                + | data=-1.2221 | grad=-0.0128\n",
      "                  * | data=-0.5330 | grad=-0.0142\n",
      "                     | data=-0.2683 | grad=0.4578\n",
      "                  + | data=-0.6890 | grad=-0.0142\n",
      "                    * | data=-0.6961 | grad=-0.0156\n",
      "                       | data=0.6954 | grad=0.1897\n",
      "                     | data=0.0070 | grad=0.0181\n",
      "      + | data=0.0221 | grad=1.0000\n",
      "        **2 | data=0.0140 | grad=1.2500\n",
      "          + | data=0.1185 | grad=0.3556\n",
      "            * | data=1.0000 | grad=0.4148\n",
      "               | data=-1.0000 | grad=-2.4991\n",
      "               | data=-1.0000 | grad=-0.4741\n",
      "            + | data=-0.8815 | grad=0.4148\n",
      "              + | data=-0.6446 | grad=0.4741\n",
      "                + | data=-0.7313 | grad=0.5333\n",
      "                  + | data=-0.0288 | grad=0.5926\n",
      "                    * | data=0.0135 | grad=0.6519\n",
      "                      tanh | data=-0.0866 | grad=-0.1118\n",
      "                        + | data=-0.0868 | grad=-0.1201\n",
      "                          + | data=-0.2227 | grad=-0.1293\n",
      "                            * | data=-0.2280 | grad=-0.1385\n",
      "                               | data=1.0000 | grad=-0.2263\n",
      "                          * | data=0.1359 | grad=-0.1293\n",
      "                             | data=-1.0000 | grad=0.7500\n",
      "                  * | data=-0.7025 | grad=0.5926\n",
      "                    tanh | data=0.8451 | grad=-0.5442\n",
      "                      + | data=1.2389 | grad=-0.1696\n",
      "                        + | data=0.3612 | grad=-0.1836\n",
      "                          * | data=0.3445 | grad=-0.1977\n",
      "                        * | data=0.8777 | grad=-0.1836\n",
      "                * | data=0.0867 | grad=0.5333\n",
      "                  tanh | data=0.3969 | grad=0.1301\n",
      "                    + | data=0.4200 | grad=0.1205\n",
      "                      + | data=0.8335 | grad=0.1314\n",
      "                        * | data=0.8416 | grad=0.1423\n",
      "                      * | data=-0.4135 | grad=0.1314\n",
      "              * | data=-0.2369 | grad=0.4741\n",
      "                tanh | data=0.7486 | grad=-0.1690\n",
      "                  + | data=0.9697 | grad=-0.0825\n",
      "                    + | data=0.7032 | grad=-0.0908\n",
      "                      * | data=0.6961 | grad=-0.0990\n",
      "                    * | data=0.2665 | grad=-0.0908\n",
      "        + | data=0.0080 | grad=1.2500\n",
      "          **2 | data=0.0080 | grad=1.5000\n",
      "            + | data=-0.0897 | grad=-0.3138\n",
      "              * | data=-1.0000 | grad=-0.3587\n",
      "                 | data=1.0000 | grad=4.7023\n",
      "                 | data=-1.0000 | grad=-0.4035\n",
      "              + | data=0.9103 | grad=-0.3587\n",
      "                * | data=-0.1699 | grad=-0.4035\n",
      "                  tanh | data=0.5368 | grad=0.1420\n",
      "                    + | data=0.5997 | grad=0.1112\n",
      "                      * | data=-0.7996 | grad=0.1213\n",
      "                         | data=3.0000 | grad=-0.4090\n",
      "                      + | data=1.3993 | grad=0.1213\n",
      "                        * | data=1.3922 | grad=0.1314\n",
      "                           | data=2.0000 | grad=0.6594\n",
      "                + | data=1.0802 | grad=-0.4035\n",
      "                  * | data=0.2173 | grad=-0.4483\n",
      "                    tanh | data=0.9942 | grad=-0.1082\n",
      "                      + | data=2.9158 | grad=-0.0014\n",
      "                        * | data=1.2406 | grad=-0.0015\n",
      "                        + | data=1.6752 | grad=-0.0015\n",
      "                          * | data=1.6833 | grad=-0.0016\n",
      "                  + | data=0.8630 | grad=-0.4483\n",
      "                    * | data=0.7968 | grad=-0.4931\n",
      "                      tanh | data=-0.9585 | grad=0.4491\n",
      "                        + | data=-1.9274 | grad=0.0395\n",
      "                          * | data=-2.6331 | grad=0.0425\n",
      "                          + | data=0.7057 | grad=0.0425\n",
      "                            * | data=0.6889 | grad=0.0456\n",
      "                    + | data=0.0662 | grad=-0.4931\n",
      "                      * | data=0.1085 | grad=-0.5380\n",
      "                        tanh | data=-0.6954 | grad=0.0916\n",
      "                          + | data=-0.8583 | grad=0.0509\n",
      "                            * | data=-0.4076 | grad=0.0545\n",
      "                            + | data=-0.4507 | grad=0.0545\n",
      "                              * | data=-0.4559 | grad=0.0582\n",
      "           | data=0.0000 | grad=1.5000\n",
      "    **2 | data=0.0000 | grad=0.7500\n",
      "      + | data=-0.0012 | grad=-0.0025\n",
      "        + | data=-1.0012 | grad=-0.0031\n",
      "          + | data=-1.1679 | grad=-0.0037\n",
      "            * | data=-0.2173 | grad=-0.0043\n",
      "              tanh | data=-0.9943 | grad=-0.0011\n",
      "                + | data=-2.9319 | grad=-0.0000\n",
      "                  + | data=-1.6913 | grad=-0.0000\n",
      "                    * | data=-1.6833 | grad=-0.0000\n",
      "                       | data=-2.0000 | grad=-0.5013\n",
      "                  * | data=-1.2406 | grad=-0.0000\n",
      "                     | data=-3.0000 | grad=0.2760\n",
      "            + | data=-0.9506 | grad=-0.0043\n",
      "              + | data=-0.1517 | grad=-0.0049\n",
      "                * | data=-0.1093 | grad=-0.0055\n",
      "                  tanh | data=0.7008 | grad=0.0010\n",
      "                    + | data=0.8688 | grad=0.0005\n",
      "                      * | data=0.4076 | grad=0.0006\n",
      "                      + | data=0.4612 | grad=0.0006\n",
      "                        * | data=0.4559 | grad=0.0006\n",
      "              * | data=-0.7990 | grad=-0.0049\n",
      "                tanh | data=0.9612 | grad=0.0046\n",
      "                  + | data=1.9609 | grad=0.0004\n",
      "                    * | data=2.6331 | grad=0.0004\n",
      "                    + | data=-0.6722 | grad=0.0004\n",
      "                      * | data=-0.6889 | grad=0.0005\n",
      "          * | data=0.1667 | grad=-0.0037\n",
      "            tanh | data=-0.5267 | grad=0.0014\n",
      "              + | data=-0.5856 | grad=0.0011\n",
      "                * | data=0.7996 | grad=0.0013\n",
      "                + | data=-1.3851 | grad=0.0013\n",
      "                  * | data=-1.3922 | grad=0.0014\n",
      "        * | data=1.0000 | grad=-0.0031\n",
      "           | data=-1.0000 | grad=0.0037\n",
      "           | data=-1.0000 | grad=-3.9006\n",
      "   | data=0.2500 | grad=0.0460\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss.backward()\n",
    "print_graph(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "473c4282-464c-420c-ae77-b5586c8a6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    correct = 0\n",
    "    for yp, yt in zip(y_pred, y_true):\n",
    "        pred = 1 if yp.data > 0 else -1\n",
    "        if pred == yt.data:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return sum((yp - yt)**2 for yp, yt in zip(y_pred, y_true)) / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1833470e-b1b2-4a7a-be9c-cafeb6b8f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss = 4.1485 | Accuracy = 25.00%\n",
      "Epoch 01 | Loss = 1.9259 | Accuracy = 50.00%\n",
      "Epoch 02 | Loss = 0.5665 | Accuracy = 100.00%\n",
      "Epoch 03 | Loss = 0.1753 | Accuracy = 100.00%\n",
      "Epoch 04 | Loss = 0.0734 | Accuracy = 100.00%\n",
      "Epoch 05 | Loss = 0.0354 | Accuracy = 100.00%\n",
      "Epoch 06 | Loss = 0.0186 | Accuracy = 100.00%\n",
      "Epoch 07 | Loss = 0.0105 | Accuracy = 100.00%\n",
      "Epoch 08 | Loss = 0.0064 | Accuracy = 100.00%\n",
      "Epoch 09 | Loss = 0.0042 | Accuracy = 100.00%\n",
      "Epoch 10 | Loss = 0.0030 | Accuracy = 100.00%\n",
      "Epoch 11 | Loss = 0.0023 | Accuracy = 100.00%\n",
      "Epoch 12 | Loss = 0.0018 | Accuracy = 100.00%\n",
      "Epoch 13 | Loss = 0.0016 | Accuracy = 100.00%\n",
      "Epoch 14 | Loss = 0.0014 | Accuracy = 100.00%\n",
      "Epoch 15 | Loss = 0.0012 | Accuracy = 100.00%\n",
      "Epoch 16 | Loss = 0.0011 | Accuracy = 100.00%\n",
      "Epoch 17 | Loss = 0.0011 | Accuracy = 100.00%\n",
      "Epoch 18 | Loss = 0.0010 | Accuracy = 100.00%\n",
      "Epoch 19 | Loss = 0.0010 | Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "model = MLP(2, [4, 1])\n",
    "epochs = 20\n",
    "\n",
    "xs = [[Value(2.0), Value(3.0)],[Value(1.0), Value(-1.0)],[Value(-1.0), Value(2.0)],[Value(-2.0), Value(-3.0)]]\n",
    "ys = [Value(1.0),Value(-1.0),Value(1.0),Value(-1.0)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = [model(x) for x in xs]\n",
    "    loss = mse_loss(y_pred, ys)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    acc = accuracy(y_pred, ys)\n",
    "    print(f\"Epoch {epoch:02d} | Loss = {loss.data:.4f} | Accuracy = {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2681f-8605-4a76-9bb0-963f0deb07ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45166456-04ed-4b41-a75f-92e1884ab696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
